{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEG0omZ1I/2w/ji9+fGu3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teratoma111/amazon_review_analysis/blob/main/active_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gnvfJNFJfMS",
        "outputId": "f6761b62-5335-4543-b509-0aae6244abd4"
      },
      "source": [
        "import torch\n",
        "from torchtext.legacy import data\n",
        "import numpy as np  \n",
        "import torch.optim as optim\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(include_lengths = True,\n",
        "                  tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL = data.LabelField()\n",
        "INDEX = data.LabelField()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import random\n",
        "\n",
        "fields = {'reviewText': ('text', TEXT), 'overall': ('label', LABEL), 'index': ('index', INDEX)}\n",
        "train_data = data.TabularDataset.splits(\n",
        "                                        path = '/content/drive/MyDrive',\n",
        "                                        train = \"Musical_Instruments_5(mod).json\",\n",
        "                                        format = 'json',\n",
        "                                        fields = fields)[0]\n",
        "    \n",
        "for i in range(0, len(train_data)):\n",
        "  if(len(train_data[i].text) < 1):\n",
        "    train_data[i].text = train_data[3].text\n",
        "    train_data[i].label = train_data[3].label\n",
        "origin_data = train_data\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQPXZ1wnQbWo"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khryjRDmQl4h"
      },
      "source": [
        "import torch.nn as nn\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHwO2nNVQsRN"
      },
      "source": [
        "def get_weights(iterator):\n",
        "  weights = torch.zeros(len(LABEL.vocab))\n",
        "  for batch in iterator:\n",
        "    for i in batch.label:\n",
        "      weights[i.cpu().numpy()] +=1\n",
        "  for i in range(0, len(LABEL.vocab)):\n",
        "    weights[i] = (1 / weights[i]) * 100\n",
        "  return weights"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ2AcVSUQuOb"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    y_score = top_pred.detach().cpu().numpy()\n",
        "    y_true = y.view_as(top_pred).cpu().numpy()\n",
        "    f1 = f1_score(y_true, y_score, average = 'weighted')\n",
        "    rec = recall_score(y_true, y_score, average = 'weighted')\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return (rec, f1, acc)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCpbQDpMQ3hW"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "    epoch_rec = 0\n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        metrics = categorical_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        acc = metrics[2]\n",
        "        f1 = metrics[1]\n",
        "        rec = metrics[0]\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_f1 += f1\n",
        "        epoch_rec += rec\n",
        "    epoch_acc /= len(iterator)\n",
        "    epoch_f1 /= len(iterator)\n",
        "    epoch_rec /= len(iterator)\n",
        "    return epoch_loss / len(iterator), (epoch_rec, epoch_f1, epoch_acc)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRqkyyuUQ5Sg"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_f1 = 0\n",
        "    epoch_rec = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            metrics = categorical_accuracy(predictions, batch.label)\n",
        "\n",
        "            acc = metrics[2]\n",
        "            f1 = metrics[1]\n",
        "            rec = metrics[0]\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_f1 += f1\n",
        "            epoch_rec += rec\n",
        "        epoch_acc /= len(iterator)\n",
        "        epoch_f1 /= len(iterator)\n",
        "        epoch_rec /= len(iterator)\n",
        "        \n",
        "    return epoch_loss / len(iterator), (epoch_rec, epoch_f1, epoch_acc)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEKzOSX3Q6s5",
        "outputId": "a6bcbb26-6233-4834-f0e9-2f845b2cd0cb"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "TEXT.build_vocab(train_data, \n",
        "                 vectors = \"fasttext.simple.300d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "INDEX.build_vocab(train_data)\n",
        "TEXT1 = data.Field(include_lengths = True,\n",
        "                  tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL1 = data.LabelField()\n",
        "INDEX1 = data.LabelField()\n",
        "field = {('text', TEXT1),('label', LABEL1), ('index', INDEX1)}\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))\n",
        "train_data_arr = np.array(train_data.examples)\n",
        "l = list(range(0, len(train_data)))\n",
        "random.shuffle(l)\n",
        "rand_data =  data.Dataset(train_data_arr[l[-200:]], fields=field)\n",
        "TEXT1.build_vocab(train_data, \n",
        "                 vectors = \"fasttext.simple.300d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL1.build_vocab(train_data)\n",
        "INDEX1.build_vocab(train_data)\n",
        "BATCH_SIZE = 128\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "train_iterator, valid_iterator= data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True, \n",
        "    device = device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/wiki.simple.vec: 293MB [00:05, 54.8MB/s]                           \n",
            "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 110588/111051 [00:10<00:00, 10347.16it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuPA4HJhRp1w"
      },
      "source": [
        "  pretrained_embeddings = TEXT.vocab.vectors\n",
        "  INPUT_DIM = len(TEXT.vocab)\n",
        "  EMBEDDING_DIM = 300\n",
        "  HIDDEN_DIM = 256\n",
        "  OUTPUT_DIM = len(LABEL.vocab)\n",
        "  N_LAYERS = 2\n",
        "  BIDIRECTIONAL = True\n",
        "  DROPOUT = 0.5\n",
        "  PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "  model = RNN(INPUT_DIM, \n",
        "              EMBEDDING_DIM, \n",
        "              HIDDEN_DIM, \n",
        "              OUTPUT_DIM, \n",
        "              N_LAYERS, \n",
        "              BIDIRECTIONAL, \n",
        "              DROPOUT, \n",
        "              PAD_IDX)\n",
        "  model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "  UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "  model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "  model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(weight =  get_weights(train_iterator))\n",
        "  #criterion = nn.CrossEntropyLoss()\n",
        "  model = model.to(device)\n",
        "  criterion = criterion.to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYRpVI4pSTjR"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "TEXT1.build_vocab(rand_data, \n",
        "                 vectors = \"fasttext.simple.300d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL1.build_vocab(rand_data)\n",
        "INDEX1.build_vocab(rand_data)\n",
        "\n",
        "\n",
        "rand_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (rand_data, valid_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    #sort_within_batch = False,\n",
        "    sort_within_batch = True, \n",
        "    device = device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6oJyBiwTiAA"
      },
      "source": [
        "import time\n",
        "from torchtext import data, datasets\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yoAAvzwR360",
        "outputId": "1fe13e43-217f-4ee3-a648-b8871fcde4e1"
      },
      "source": [
        "N_EPOCHS = 15\n",
        "best_valid_score = float('0')\n",
        "best_f1 = float('0')\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_res = train(model, rand_iterator, optimizer, criterion)\n",
        "    train_loss = train_res[0]\n",
        "    train_score = train_res[1]\n",
        "    valid_res = evaluate(model, valid_iterator, criterion)\n",
        "    valid_score = valid_res[1]\n",
        "    valid_loss = valid_res[0]\n",
        "    end_time = time.time()\n",
        "    valid_acc = (valid_score[1])\n",
        "    acc = valid_acc\n",
        "    #print(float(valid_acc))\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    if acc > best_valid_score:\n",
        "        best_valid_score= acc\n",
        "        best_f1 = valid_score[1]*100\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/mo1.pt')\n",
        "        print(f'\\tValid Loss: {valid_loss:.3f} | Valid F1: {valid_score[1]*100:.2f}% | Valid rec: {valid_score[0]*100:.2f}% | Valid acc: {valid_score[2]*100:.2f}%')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train F1: {train_score[1]*100:.2f}% | Train rec: {train_score[0]*100:.2f}% | Train acc: {train_score[2]*100:.2f}%')\n",
        "print(\"--------------------------------\")\n",
        "print(best_valid_score, best_f1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 1s\n",
            "\tValid Loss: 1.610 | Valid F1: 51.31% | Valid rec: 61.03% | Valid acc: 61.03%\n",
            "\tTrain Loss: 1.603 | Train F1: 47.80% | Train rec: 46.01% | Train acc: 46.01%\n",
            "Epoch: 02 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.582 | Train F1: 51.77% | Train rec: 52.30% | Train acc: 52.30%\n",
            "Epoch: 03 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.560 | Train F1: 40.58% | Train rec: 34.33% | Train acc: 34.33%\n",
            "Epoch: 04 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.525 | Train F1: 11.85% | Train rec: 10.07% | Train acc: 10.07%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 110588/111051 [00:30<00:00, 10347.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.500 | Train F1: 3.11% | Train rec: 4.73% | Train acc: 4.73%\n",
            "Epoch: 06 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.475 | Train F1: 1.75% | Train rec: 4.04% | Train acc: 4.04%\n",
            "Epoch: 07 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.405 | Train F1: 5.55% | Train rec: 6.12% | Train acc: 6.12%\n",
            "Epoch: 08 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.252 | Train F1: 28.75% | Train rec: 22.53% | Train acc: 22.53%\n",
            "Epoch: 09 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 1.095 | Train F1: 46.59% | Train rec: 40.97% | Train acc: 40.97%\n",
            "Epoch: 10 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.949 | Train F1: 52.29% | Train rec: 46.66% | Train acc: 46.66%\n",
            "Epoch: 11 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.812 | Train F1: 56.71% | Train rec: 53.99% | Train acc: 53.99%\n",
            "Epoch: 12 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.683 | Train F1: 53.94% | Train rec: 51.30% | Train acc: 51.30%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.587 | Train F1: 64.10% | Train rec: 62.46% | Train acc: 62.46%\n",
            "Epoch: 14 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.538 | Train F1: 60.35% | Train rec: 58.68% | Train acc: 58.68%\n",
            "Epoch: 15 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.522 | Train F1: 74.36% | Train rec: 72.96% | Train acc: 72.96%\n",
            "--------------------------------\n",
            "0.5130837266324801 51.308372663248015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew-QKyVwSjTE"
      },
      "source": [
        "from torchtext.legacy import data\n",
        "train_iterator, valid_iterator= data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch = True, \n",
        "    device = device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P9mHYLCS9gT"
      },
      "source": [
        "def get_train_data(model, train_iterator, train_data):  \n",
        "  bf = [(0, 0.000001)]\n",
        "  a = np.array(bf,dtype = [('index', int) , ('prob', float)])\n",
        "  with torch.no_grad():\n",
        "    for batch in train_iterator:\n",
        "      text, text_lengths = batch.text\n",
        "      preds = model(text, text_lengths)\n",
        "      preds_sorted, idxs = preds.sort(descending=True)\n",
        "      U = (preds_sorted[:, 0] - preds_sorted[:,1])\n",
        "      for i in range(0, len(batch)):\n",
        "        j = i\n",
        "        hu = float(U.cpu().numpy()[j])\n",
        "        nuf = int(batch.index.cpu().numpy()[i])\n",
        "        b = np.array([(nuf, hu)],dtype = [('index', int) , ('prob', float)])\n",
        "        a = np.concatenate((a, b))\n",
        "    a = np.array(a,dtype = [('index', int) , ('prob', float)])\n",
        "    a.sort(order = 'prob')\n",
        "    train_data_arr = np.array(train_data.examples)\n",
        "    inds = a[-200:]['index']\n",
        "    train_id = []\n",
        "    for i_1 in inds:\n",
        "      for i_2 in range(0, len(train_data_arr)):\n",
        "        print\n",
        "        if( i_1 == vars(train_data_arr[i_2])['index']):\n",
        "          train_id.append(i_2)\n",
        "    return train_id"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfGDEIHG-Oar"
      },
      "source": [
        "def get_train_entropy_data(model, train_iterator, train_data):  \n",
        "  bf = [(0, 0.000001)]\n",
        "  a = np.array(bf,dtype = [('index', int) , ('prob', float)])\n",
        "  with torch.no_grad():\n",
        "    for batch in train_iterator:\n",
        "      text, text_lengths = batch.text\n",
        "      preds = model(text, text_lengths)\n",
        "      #preds_sorted, idxs = preds.sort(descending=True)\n",
        "      log_probs = torch.log(preds)\n",
        "      U = (preds*log_probs).sum(1)\n",
        "      for i in range(0, len(batch)):\n",
        "        j = i\n",
        "        hu = float(U.cpu().numpy()[j])\n",
        "        nuf = int(batch.index.cpu().numpy()[i])\n",
        "        b = np.array([(nuf, hu)],dtype = [('index', int) , ('prob', float)])\n",
        "        a = np.concatenate((a, b))\n",
        "    a = np.array(a,dtype = [('index', int) , ('prob', float)])\n",
        "    a.sort(order = 'prob')\n",
        "    train_data_arr = np.array(train_data.examples)\n",
        "    inds = a[-200:]['index']\n",
        "    train_id = []\n",
        "    for i_1 in inds:\n",
        "      for i_2 in range(0, len(train_data_arr)):\n",
        "        print\n",
        "        if( i_1 == vars(train_data_arr[i_2])['index']):\n",
        "          train_id.append(i_2)\n",
        "    return train_id"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hFa6-wq4bCTG",
        "outputId": "b70a52b4-498d-49de-e8ef-1d80dc7f9631"
      },
      "source": [
        "TEXT2 = data.Field(include_lengths = True,\n",
        "                  tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL2 = data.LabelField()\n",
        "INDEX2 = data.LabelField()\n",
        "best_plot = []\n",
        "for k in range(0, 10):\n",
        "  field1 = {('text', TEXT2),('label', LABEL2), ('index', INDEX2)}\n",
        "  to_data =  data.Dataset(train_data_arr[get_train_entropy_data(model, train_iterator, train_data)], fields=field1)\n",
        "  TEXT2.build_vocab(to_data, \n",
        "                  vectors = \"fasttext.simple.300d\", \n",
        "                  unk_init = torch.Tensor.normal_)\n",
        "  LABEL2.build_vocab(to_data)\n",
        "  INDEX2.build_vocab(train_data)\n",
        "  INDEX2.build_vocab(to_data)\n",
        "  to_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "      (to_data, valid_data), \n",
        "      batch_size = BATCH_SIZE,\n",
        "      sort_key = lambda x: len(x.text),\n",
        "      sort_within_batch = True, \n",
        "      device = device)\n",
        "  N_EPOCHS = 12\n",
        "  best_valid_score = float('0')\n",
        "  best_f1 = float('0')\n",
        "  for epoch in range(N_EPOCHS):\n",
        "      start_time = time.time()\n",
        "      train_res = train(model, to_iterator, optimizer, criterion)\n",
        "      train_loss = train_res[0]\n",
        "      train_score = train_res[1]\n",
        "      valid_res = evaluate(model, train_iterator, criterion)\n",
        "      valid_score = valid_res[1]\n",
        "      valid_loss = valid_res[0]\n",
        "      end_time = time.time()\n",
        "      valid_acc = (valid_score[1])\n",
        "      acc = valid_acc\n",
        "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "      if acc > best_valid_score:\n",
        "          best_valid_score= acc\n",
        "          best_f1 = valid_score[1]*100\n",
        "          torch.save(model.state_dict(), '/content/drive/MyDrive/mo1.pt')\n",
        "          print(f'\\tValid Loss: {valid_loss:.3f} | Valid F1: {valid_score[1]*100:.2f}% | Valid rec: {valid_score[0]*100:.2f}% | Valid acc: {valid_score[2]*100:.2f}%')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train F1: {train_score[1]*100:.2f}% | Train rec: {train_score[0]*100:.2f}% | Train acc: {train_score[2]*100:.2f}%')\n",
        "  print(\"--------------------------------\")\n",
        "  best_plot.append(best_f1)\n",
        "  print(best_valid_score, best_f1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 2.290 | Valid F1: 10.97% | Valid rec: 11.76% | Valid acc: 11.76%\n",
            "\tTrain Loss: 2.825 | Train F1: 22.97% | Train rec: 17.84% | Train acc: 17.84%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 2.066 | Train F1: 12.99% | Train rec: 13.59% | Train acc: 13.59%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.822 | Train F1: 7.45% | Train rec: 11.24% | Train acc: 11.24%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 1.852 | Valid F1: 12.68% | Valid rec: 11.16% | Valid acc: 11.16%\n",
            "\tTrain Loss: 1.753 | Train F1: 2.34% | Train rec: 6.42% | Train acc: 6.42%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.265 | Train F1: 14.89% | Train rec: 17.32% | Train acc: 17.32%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.259 | Train F1: 16.94% | Train rec: 18.53% | Train acc: 18.53%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.207 | Train F1: 13.14% | Train rec: 14.84% | Train acc: 14.84%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 1.097 | Train F1: 18.19% | Train rec: 21.66% | Train acc: 21.66%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 1.742 | Valid F1: 17.05% | Valid rec: 16.68% | Valid acc: 16.68%\n",
            "\tTrain Loss: 1.075 | Train F1: 20.42% | Train rec: 21.74% | Train acc: 21.74%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 1.760 | Valid F1: 26.52% | Valid rec: 23.62% | Valid acc: 23.62%\n",
            "\tTrain Loss: 0.990 | Train F1: 27.77% | Train rec: 28.65% | Train acc: 28.65%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 1.791 | Valid F1: 37.26% | Valid rec: 31.86% | Valid acc: 31.86%\n",
            "\tTrain Loss: 0.962 | Train F1: 36.61% | Train rec: 33.42% | Train acc: 33.42%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 1.818 | Valid F1: 43.64% | Valid rec: 38.45% | Valid acc: 38.45%\n",
            "\tTrain Loss: 0.902 | Train F1: 47.00% | Train rec: 43.53% | Train acc: 43.53%\n",
            "--------------------------------\n",
            "0.4363575285263666 43.635752852636664\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 1.858 | Valid F1: 44.72% | Valid rec: 39.93% | Valid acc: 39.93%\n",
            "\tTrain Loss: 0.841 | Train F1: 50.27% | Train rec: 46.92% | Train acc: 46.92%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.782 | Train F1: 53.14% | Train rec: 50.69% | Train acc: 50.69%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.736 | Train F1: 51.36% | Train rec: 49.39% | Train acc: 49.39%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.695 | Train F1: 47.53% | Train rec: 44.79% | Train acc: 44.79%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.669 | Train F1: 45.79% | Train rec: 42.49% | Train acc: 42.49%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.624 | Train F1: 47.28% | Train rec: 44.57% | Train acc: 44.57%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.624 | Train F1: 54.57% | Train rec: 53.17% | Train acc: 53.17%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.747 | Train F1: 46.76% | Train rec: 41.93% | Train acc: 41.93%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 2.429 | Valid F1: 53.42% | Valid rec: 58.20% | Valid acc: 58.20%\n",
            "\tTrain Loss: 0.624 | Train F1: 59.20% | Train rec: 59.03% | Train acc: 59.03%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 2.530 | Valid F1: 54.51% | Valid rec: 60.26% | Valid acc: 60.26%\n",
            "\tTrain Loss: 0.588 | Train F1: 65.96% | Train rec: 68.71% | Train acc: 68.71%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.688 | Train F1: 65.54% | Train rec: 67.23% | Train acc: 67.23%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.533 | Train F1: 67.07% | Train rec: 64.76% | Train acc: 64.76%\n",
            "--------------------------------\n",
            "0.5450675382698498 54.506753826984976\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 2.502 | Valid F1: 35.78% | Valid rec: 29.74% | Valid acc: 29.74%\n",
            "\tTrain Loss: 0.538 | Train F1: 58.18% | Train rec: 54.51% | Train acc: 54.51%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 2.553 | Valid F1: 43.14% | Valid rec: 37.97% | Valid acc: 37.97%\n",
            "\tTrain Loss: 0.512 | Train F1: 53.92% | Train rec: 50.91% | Train acc: 50.91%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 2.644 | Valid F1: 50.44% | Valid rec: 48.94% | Valid acc: 48.94%\n",
            "\tTrain Loss: 0.469 | Train F1: 63.44% | Train rec: 61.33% | Train acc: 61.33%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 2.742 | Valid F1: 51.68% | Valid rec: 51.81% | Valid acc: 51.81%\n",
            "\tTrain Loss: 0.464 | Train F1: 66.53% | Train rec: 66.06% | Train acc: 66.06%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.434 | Train F1: 68.73% | Train rec: 68.19% | Train acc: 68.19%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.426 | Train F1: 67.31% | Train rec: 66.58% | Train acc: 66.58%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 3.185 | Valid F1: 53.23% | Valid rec: 54.64% | Valid acc: 54.64%\n",
            "\tTrain Loss: 0.403 | Train F1: 72.13% | Train rec: 72.44% | Train acc: 72.44%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.364 | Train F1: 74.75% | Train rec: 74.00% | Train acc: 74.00%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.328 | Train F1: 73.38% | Train rec: 71.88% | Train acc: 71.88%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.327 | Train F1: 66.68% | Train rec: 63.85% | Train acc: 63.85%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.288 | Train F1: 68.62% | Train rec: 66.32% | Train acc: 66.32%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.272 | Train F1: 76.66% | Train rec: 75.91% | Train acc: 75.91%\n",
            "--------------------------------\n",
            "0.5322678241319655 53.22678241319655\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 3.658 | Valid F1: 52.75% | Valid rec: 53.40% | Valid acc: 53.40%\n",
            "\tTrain Loss: 0.263 | Train F1: 78.21% | Train rec: 77.95% | Train acc: 77.95%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.214 | Train F1: 86.15% | Train rec: 85.72% | Train acc: 85.72%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.234 | Train F1: 77.64% | Train rec: 75.78% | Train acc: 75.78%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.207 | Train F1: 79.07% | Train rec: 77.47% | Train acc: 77.47%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 4.091 | Valid F1: 53.53% | Valid rec: 56.48% | Valid acc: 56.48%\n",
            "\tTrain Loss: 0.188 | Train F1: 88.23% | Train rec: 88.28% | Train acc: 88.28%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.203 | Train F1: 83.15% | Train rec: 82.94% | Train acc: 82.94%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.174 | Train F1: 82.54% | Train rec: 80.51% | Train acc: 80.51%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.174 | Train F1: 88.13% | Train rec: 88.50% | Train acc: 88.50%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.142 | Train F1: 91.07% | Train rec: 90.67% | Train acc: 90.67%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.162 | Train F1: 90.31% | Train rec: 89.71% | Train acc: 89.71%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.154 | Train F1: 92.62% | Train rec: 92.75% | Train acc: 92.75%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.134 | Train F1: 86.85% | Train rec: 85.89% | Train acc: 85.89%\n",
            "--------------------------------\n",
            "0.5353276422494581 53.53276422494581\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 4.410 | Valid F1: 54.77% | Valid rec: 59.31% | Valid acc: 59.31%\n",
            "\tTrain Loss: 0.131 | Train F1: 89.79% | Train rec: 89.11% | Train acc: 89.11%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.105 | Train F1: 95.38% | Train rec: 95.49% | Train acc: 95.49%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.084 | Train F1: 96.01% | Train rec: 95.75% | Train acc: 95.75%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.091 | Train F1: 94.27% | Train rec: 94.01% | Train acc: 94.01%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.058 | Train F1: 98.94% | Train rec: 98.91% | Train acc: 98.91%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.050 | Train F1: 99.22% | Train rec: 99.22% | Train acc: 99.22%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.044 | Train F1: 97.89% | Train rec: 97.83% | Train acc: 97.83%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.045 | Train F1: 97.33% | Train rec: 97.27% | Train acc: 97.27%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.023 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.029 | Train F1: 99.62% | Train rec: 99.61% | Train acc: 99.61%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.023 | Train F1: 98.85% | Train rec: 98.83% | Train acc: 98.83%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.021 | Train F1: 99.23% | Train rec: 99.22% | Train acc: 99.22%\n",
            "--------------------------------\n",
            "0.5477196389693848 54.77196389693848\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 4.902 | Valid F1: 52.95% | Valid rec: 55.29% | Valid acc: 55.29%\n",
            "\tTrain Loss: 0.018 | Train F1: 99.61% | Train rec: 99.61% | Train acc: 99.61%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.013 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.011 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.010 | Train F1: 99.62% | Train rec: 99.61% | Train acc: 99.61%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.012 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.012 | Train F1: 99.62% | Train rec: 99.61% | Train acc: 99.61%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.020 | Train F1: 98.85% | Train rec: 98.83% | Train acc: 98.83%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.010 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.008 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.009 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.013 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.007 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "--------------------------------\n",
            "0.5294640171956383 52.94640171956383\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 4.951 | Valid F1: 51.60% | Valid rec: 52.05% | Valid acc: 52.05%\n",
            "\tTrain Loss: 0.011 | Train F1: 99.32% | Train rec: 99.31% | Train acc: 99.31%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.405 | Valid F1: 53.97% | Valid rec: 57.58% | Valid acc: 57.58%\n",
            "\tTrain Loss: 0.008 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.418 | Valid F1: 54.31% | Valid rec: 58.45% | Valid acc: 58.45%\n",
            "\tTrain Loss: 0.022 | Train F1: 99.30% | Train rec: 99.31% | Train acc: 99.31%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.006 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.007 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.016 | Train F1: 99.25% | Train rec: 99.22% | Train acc: 99.22%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.012 | Train F1: 98.94% | Train rec: 98.91% | Train acc: 98.91%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.008 | Train F1: 99.32% | Train rec: 99.31% | Train acc: 99.31%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.006 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.004 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.796 | Valid F1: 54.49% | Valid rec: 59.53% | Valid acc: 59.53%\n",
            "\tTrain Loss: 0.007 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.868 | Valid F1: 54.56% | Valid rec: 59.61% | Valid acc: 59.61%\n",
            "\tTrain Loss: 0.004 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "--------------------------------\n",
            "0.5456046989987465 54.560469899874654\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.743 | Valid F1: 54.00% | Valid rec: 57.93% | Valid acc: 57.93%\n",
            "\tTrain Loss: 0.005 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.006 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.007 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.007 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.007 | Train F1: 99.61% | Train rec: 99.61% | Train acc: 99.61%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.005 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.003 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.003 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.824 | Valid F1: 54.09% | Valid rec: 58.95% | Valid acc: 58.95%\n",
            "\tTrain Loss: 0.004 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.003 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.003 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.008 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "--------------------------------\n",
            "0.5408861215840379 54.08861215840379\n",
            "Epoch: 01 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.192 | Valid F1: 45.66% | Valid rec: 43.23% | Valid acc: 43.23%\n",
            "\tTrain Loss: 0.003 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 02 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 4.992 | Valid F1: 49.83% | Valid rec: 49.10% | Valid acc: 49.10%\n",
            "\tTrain Loss: 0.050 | Train F1: 99.22% | Train rec: 99.22% | Train acc: 99.22%\n",
            "Epoch: 03 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.004 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 04 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.298 | Valid F1: 51.62% | Valid rec: 54.09% | Valid acc: 54.09%\n",
            "\tTrain Loss: 0.030 | Train F1: 98.88% | Train rec: 98.83% | Train acc: 98.83%\n",
            "Epoch: 05 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.606 | Valid F1: 53.80% | Valid rec: 57.48% | Valid acc: 57.48%\n",
            "\tTrain Loss: 0.006 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 06 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.715 | Valid F1: 54.23% | Valid rec: 57.34% | Valid acc: 57.34%\n",
            "\tTrain Loss: 0.006 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 07 | Epoch Time: 0m 2s\n",
            "\tValid Loss: 5.761 | Valid F1: 54.40% | Valid rec: 57.86% | Valid acc: 57.86%\n",
            "\tTrain Loss: 0.017 | Train F1: 99.61% | Train rec: 99.61% | Train acc: 99.61%\n",
            "Epoch: 08 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.007 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 09 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.003 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 10 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.006 | Train F1: 100.00% | Train rec: 100.00% | Train acc: 100.00%\n",
            "Epoch: 11 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.006 | Train F1: 99.62% | Train rec: 99.61% | Train acc: 99.61%\n",
            "Epoch: 12 | Epoch Time: 0m 2s\n",
            "\tTrain Loss: 0.009 | Train F1: 99.32% | Train rec: 99.31% | Train acc: 99.31%\n",
            "--------------------------------\n",
            "0.5440431941810235 54.404319418102354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8fedf214997f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mfield1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABEL2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINDEX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mto_data\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_train_entropy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   TEXT2.build_vocab(to_data, \n\u001b[1;32m     11\u001b[0m                   \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"fasttext.simple.300d\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-84441e6c24bc>\u001b[0m in \u001b[0;36mget_train_entropy_data\u001b[0;34m(model, train_iterator, train_data)\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi_2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mi_1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m           \u001b[0mtrain_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXRtoU5bcG5B"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/mo_feated_entropy.pt')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "HLVEXbJvkB8X",
        "outputId": "e2f1a679-49b3-48cd-8d34-a3c714512f02"
      },
      "source": [
        "print(best_plot)\n",
        "x_point = []\n",
        "for temp in (range(1,10)):\n",
        "  x_point.append(temp)\n",
        "print(x_point)\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_point, best_plot)\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[43.635752852636664, 54.506753826984976, 53.22678241319655, 53.53276422494581, 54.77196389693848, 52.94640171956383, 54.560469899874654, 54.08861215840379, 54.404319418102354]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qoZquyVM6nF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}